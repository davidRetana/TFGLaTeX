\chapter{Machine Learning}
Los datos son una fuente de valor por lo que analizarlos y tomar decisiones en función a estos se ha
convertido en algo esencial. Cada vez son más las compañías autodenominadas 
\textit{data-driven company}\index{Data-driven company}, es decir, empresas que toman decisiones de 
futuro e inversión en función al análisis que hacen de sus datos.\\
El \textit{machine learning} se nutre de los datos ya que permite construir modelos sobre estos,
que posteriormente se usarán para tomar dichas decisiones de futuro de la compañía. A modo de ejemplo, estas
decisiones pueden ser: ¿Dónde construir un nuevo centro de datos?, ¿Cómo me anticipo a la posible baja de un
cliente de mis servicios móviles?, ¿Cómo minimizo el coste de mantenimiento de las infraestructuras?...\\
Por estas y más razones, el \textit{machine learning} y las grandes cantidades de datos manejadas en el 
\textit{Big Data} están estrechamente relacionadas.

\section{¿Qué es el machine learning?}
El \textit{Machine Learning}\index{Machine Learning} (ML) es una rama de la inteligencia artificial (IA) 
que se centra en desarrollar métodos para hacer posible que los sistemas aprendan sin ser programados 
explícitamente para ello.
El denominado aprendizaje automático se basa en los datos de entrada o datos de entrenamiento, que
son utilizados para ajustar los modelos tanto predictivos, como clasificatorios o de clusterización.\\
Podemos clasificar los modelos de \textit{machine learning} en dos clases:
\begin{itemize}
  \item \textbf{Aprendizaje supervisado}\index{Aprendizaje!supervisado}: los datos de entrada del 
  algoritmo van etiquetados con la salida esperada, es decir, cada dato de entrada lleva consigo 
  la clase a la que pertenece. De esta manera se pretende que el algoritmo sea capaz de identificar 
  patrones entre los datos de una misma clase para que cuando vea un dato sin etiquetar, este sea 
  capaz de asignarle una clase en función de los patrones aprendidos en la fase de entrenamiento.
  Unos ejemplos de esta clase de algoritmos serían: regresión lineal, regresión logística, 
  \textit{support vector machine}, redes neuronales...
  
  \item \textbf{Aprendizaje no supervisado}\index{Aprendizaje!no supervisado}: los datos pasados 
  al algoritmo no llevan asignados una etiqueta, es el propio algoritmo el que debe aprender 
  patrones similares entre todo el conjunto de datos de entrada.\\
  Algunos ejemplos de aprendizaje no supervisado serían: \textit{KMeans}, \textit{Principal Component Analysis}, 
  \textit{k-nearest neighbors}...
\end{itemize}

Aunque el concepto de \textit{machine learning} apareció a mediados del siglo $XX$, es ahora cuando su evolución
ha crecido en importancia debido al aumento de la capacidad de computo de los ordenadores.
Lo esencial en \textit{machine learning} son los datos, los modelos se alimentan de ellos y es por lo que cuantos más
datos tengamos a disposición, más preciso será nuestro modelo entrenado a coste de un mayor
tiempo de procesamiento (\cite{DBLP:books/lib/Bishop07}). Esta afirmación es matizable, ya que en ciertos casos 
(ver: \url{https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff})
la inclusión de más datos a tu algoritmo no aumentara su desempeño.
\newline

El ML tiene campos de aplicación muy diversos tales como procesamiento del lenguaje natural, robótica,
desarrollo de motores de búsqueda y recomendación, detección de fraude...\\

\section[\textit{Machine Learning} distribuido]{¿Por qué desarrollar \textit{machine learning} de manera distribuida?}
En la era de la generación masiva de datos, el mejor aliado del \textit{Big Data} es el \textit{machine learning}.
Gracias a esto podemos generar modelos artificiales en muy diversas áreas para sacar un beneficio
mayor a nuestros datos. Podemos crear modelos predictivos, probabilísticos, clasificatorios,
sistemas de detección de fraude y anomalías, algoritmos de compresión de datos...
\newline

Para entrenar estos modelos necesitamos datos, tradicionalmente, este proceso de entrenamiento
del algoritmo se hacía de manera secuencial en una sola máquina. Hay muchos lenguajes de programación, 
librerías y herramientas para llevar a cabo este proceso de entrenamiento y modelización como por
ejemplo \textit{R}, \textit{Matlab}, \textit{Octave} o \textit{Python} mediante la librería 
\href{http://scikit-learn.org/stable/}{\textit{sklearn}}.
Estas herramientas funcionan muy bien pero solo en conjuntos pequeños de datos.
\newline

Si queremos desarrollar un modelo con un buen rendimiento necesitamos muchos datos, tantos que a veces
superan ampliamente la memoria disponible del ordenador así como la capacidad de los procesadores para
poder procesar toda la información en un tiempo razonable.\\
A medida que el conjunto de datos de entrenamiento crece, las herramientas tradicionales se quedan
inservibles para este propósito, por esta razón, estamos en la necesidad de desarrollar algoritmos
paralelos que den solución a estos problemas.
\newline

En el artículo de investigación \cite{NIPS2006_3150}, se puede comprobar los efectos de la paralelización
(en cuanto a términos de rendimiento) de algunos algoritmo de \textit{machine learning}. Esto pone de
manifiesto más aún si cabe la necesidad de distribuir los cómputos entre varias máquinas. Como se ve en
la \autoref{sec:ml_pipeline}, el proceso de creación de un modelo de \textit{machine learning} se basa en
la prueba y error por lo que normalmente es necesario entrenar el modelo sobre el mismo conjunto de datos
varias veces, hasta que se obtengan las métricas deseadas. Esto aumenta más aún si cabe el tiempo necesario
de entrenamiento, que en entornos secuenciales pueden llegar a ser inviables.
\newline

En el \autoref{chap:implementacion_paralela} se estudiará, analizará e implementará de manera paralela 
algunos de los algoritmos más populares y usados hoy en día de \textit{machine learning}.

\section{Machine learning pipelines}\label{sec:ml_pipeline}
El desarrollo de un modelo de \textit{machine learning} pasa por una serie de fases o procesos de desarrollo,
desde el preprocesamiento de los datos crudos hasta la utilización de dicho modelo en un entorno
de producción.

En la vida real los datos provienen de diversas fuentes como sensores, redes sociales o bases de datos.
Estos datos no siempre vienen formateados de manera numérica por lo que
hay diversos motivos por los cuales el preprocesamiento es algo fundamental.\\
Nos podemos encontrar con que nos faltan valores en un determinado registro y una determinada columna
que deben ser rellenados o eliminados con el fin de utilizarlos posteriormente en nuestro modelo.
En los datos crudos suele ser muy común encontrarse con características que son categóricas en 
vez de numéricas, esto es, un registro puede contener el sexo de una persona (hombre, mujer), el color 
de un coche (rojo, negro, azul...). 
Todas estas características deben ser convertidas a un formato numérico (escalar o vector de escalares) 
para ser procesadas posteriormente.\\
También puede ser que nuestro \textit{dataset} crudo tal vez contenga 
\textit{outliers}\index{Outlier}\footnote{los outliers son valores atípicos  respecto al resto de observaciones 
de una muestra.} que queramos eliminar o transformar, etc.
\newline

Debido a estos motivos, necesitamos un preprocesamiento de los datos con el fin de limpiarlos y 
prepararlos para ser consumidos por el modelo matemático.\\
Posteriormente, en la mayoría de los casos es necesario escalar los datos para un desempeño óptimo, 
debido a que si no lo hacemos, las características de los datos con un valor numérico más alto 
tendrán más peso en la función objetivo. Este escalado previo es necesario en modelos como 
las redes neuronales o la regresión logística.

\clearpage

\noindent De manera esquemática, los pasos a seguir son:
\begin{framed}
\begin{enumerate}
  \item \textbf{Preprocesamiento}
  \begin{enumerate}
    \item Limpieza y purga: quitar o rellenar registros vacíos, tratamiento de valores nulos 
          y $N/A$, convenciones en formato de fechas...
    \item \textit{Feature Engineering}\index{Feature engineering}: convertir variables categóricas a numéricas, 
          estandarización o escalado de los datos...
    \item División de los datos: dividir el conjunto de datos en conjunto de entrenamiento y 
          conjunto de test (generalmente en proporciones $70 - 30\%$ o similar).
  \end{enumerate}
  
  \item \textbf{Procesamiento}
  \begin{enumerate}
    \item Elección de modelo que mejor se ajuste a nuestro problema 
          (\textit{Support Vector Machine}, Redes Neuronales, regresión lineal, etc.)
    \item Entrenamiento: \textit{Hyperparameter tuning}\index{Hyperparameter tuning}, 
          es decir, ajuste de los parámetros variables del modelo (regularización, grado, tolerancia...).
    \item Testar el modelo con el conjunto de datos de test. Esto nos arroja las métricas necesarias
    para identificar cuan de bien se desempeña nuestro algoritmo en datos nunca vistos antes 
    (precisión, \textit{recall}\index{Recall}, matriz de confusión...)
  \end{enumerate}
  
  \item \textbf{Producción}
  \begin{enumerate}
    \item Preprocesamiento usando los mismos criterios que en la primera etapa con datos nunca 
          antes vistos.
    \item Aplicación del modelo ya entrenado a los datos anteriores para sacar los resultados.
  \end{enumerate}
\end{enumerate}
\end{framed}

A pesar de que las fórmulas matemáticas en las que se apoyan los modelos de \textit{machine learning} son
exactas, el proceso de creación de un modelo no es una ciencia exacta o no hay unas reglas universales. 
El desarrollo se basa en la experimentación a base de prueba y error sobre el conjunto de datos de entrenamiento.
Si las métricas que obtenemos al entrenar un modelo no son las deseadas, no hay un teorema que nos indique
exactamente donde se puede mejorar el modelo, debemos ser nosotros como programadores los que tengamos que
decidir que hacer.\hfill \break
En la \autoref{ml_pipeline} se muestra el flujo de desarrollo de un proyecto de \textit{machine learning}.

\begin{figure}[h]
  \centering
  \textbf{\textbf{\textit{Pipeline}}}\par\medskip
  \includegraphics[width=\textwidth]{C:/Users/David/Desktop/TFG/TFGLatex/imagenes/ml_pipeline.jpg}
  \caption[\textit{Machine learning pipeline}]{Diagrama de flujo de un proyecto de \textit{machine learning}}
  \label{ml_pipeline}
\end{figure}

\clearpage


\chapter[Implementación paralela]{Implementación paralela de algoritmos}\label{chap:implementacion_paralela}
En este capítulo se implementarán algunos de los algoritmos más populares de \textit{machine learning} 
pero con un enfoque distribuido, con el fin de minimizar los tiempos de ejecución a medida que
el \textit{dataset} se vuelve más grande.
Cada sección consistirá en explicar de una manera breve y concisa la problemática a estudiar, 
el algoritmo a desarrollar y la publicación del código fuente del algoritmo en paralelo.
\newline

Para comenzar, vamos a establecer una serie de convenciones para la notación, con el fin de explicar
las matemáticas que hay detrás de los algoritmos de \textit{machine learning}.\\
El numero total de datos de entrenamiento se marcara con la letra $m$, mientras que es numero total
de características de los datos se denotara con la letra $n$.\\
Un dato de entrenamiento sera un vector fila donde cada componente del vector será una característica
de dicho dato. A cada dato del conjunto de entrenamiento se le denotara con la letra $x$ y un 
superíndice, mientras que las características se denotaran con la letra $x$ y subíndices.\\ 
En caso de que el dato lleve aparejado una clase a la que pertenece, ésta será denotada con la letra 
$y$. Así pues, para un primer ejemplo de entrenamiento (supervisado) sería:
$$ x^{(1)}=(\underbrace{x^{(1)}_1, x^{(1)}_2, \cdots, x^{(1)}_n}_{\text{n características}}) \in \mathds{R}^n; \>
   y^{(1)} \in \mathds{R} $$

Al conjunto total de datos lo denotaremos con la letra $X$ que representará la matriz que contiene
todos los datos de entrenamiento, donde cada ejemplo será un vector fila de la matriz.
La letra $y$ será un vector columna conteniendo las clases de sus respectivos datos (aparejados por el índice), 
por lo que a la fila $i$-ésima de la matriz le corresponde la clase $i$-ésima del vector $y$.
Si estamos hablando de un problema de aprendizaje no supervisado, no tendría sentido hablar de las clases 
aparejadas a cada dato, con lo cual el vector $y$ no existiría.\\
A modo de ejemplo, el conjunto de entrenamiento para un problema de aprendizaje supervisado quedaría así:

$$
X = \left(\begin{array}{ccc}
    \textemdash & x^{(1)} & \textemdash \\
    \textemdash & x^{(2)} & \textemdash \\
     & \vdots &  \\
    \textemdash & x^{(m)} & \textemdash \\
\end{array}\right)
%
=
  \left(\begin{array}{ccccc}
    x^{(1)}_1 & x^{(1)}_2 & x^{(1)}_3 & \cdots & x^{(1)}_n \\
    x^{(2)}_1 & x^{(2)}_2 & x^{(2)}_3 & \cdots & x^{(2)}_n \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x^{(m)}_1 & x^{(m)}_2 & x^{(m)}_3 & \cdots & x^{(m)}_n \\
\end{array}\right)
%
;
y = \left(\begin{array}{c}
    y^{(1)} \\
    y^{(2)} \\
    \vdots \\
    y^{(m)} \\
\end{array}\right)
$$

De manera análoga si hubiera que hacer distinción entre los datos de entrenamiento y los datos de test, 
se denotarán $X_{train}, y_{train}$ y $X_{test}, y_{test}$.


\section{Aprendizaje supervisado}
En el aprendizaje supervisado necesitamos que cada dato de entrenamiento lleve aparejado una clase 
a la que pertenece. De esta manera nuestro algoritmo intentara adaptarse a los datos lo mejor 
posible corrigiendo los errores en función de la clase real de cada ejemplo.
En esta sección se estudiaran los algoritmos de Regresión Lineal y \textit{NaiveBayes}.

\clearpage

\input{C:/Users/David/Desktop/TFG/TFGLatex/analisis_datos/aprendizaje_supervisado}


\section{Aprendizaje no supervisado}
En el aprendizaje no supervisado es el propio algoritmo el que debe sacar los patrones de 
comportamiento de todo el conjunto de datos.
En esta sección se estudiaran los algoritmos de Detección de anomalías y \textit{K-Means}.
\input{C:/Users/David/Desktop/TFG/TFGLatex/analisis_datos/aprendizaje_no_supervisado}
